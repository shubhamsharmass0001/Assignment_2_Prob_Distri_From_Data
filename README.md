
# ğŸ“˜ Assignmentâ€“2  
## Learning Probability Density Functions Using Data Only (GAN-Based Approach)

---

## ğŸ“Œ Overview

This repository contains the complete implementation of **Assignmentâ€“2**, where the goal is to **learn an unknown probability density function (PDF)** of a transformed random variable using a **Generative Adversarial Network (GAN)**.

No analytical or parametric form of the probability density function is assumed.  
The distribution is learned **only from data samples**, demonstrating **implicit density modeling using GANs**.

---

## ğŸ¯ Assignment Objective

The objectives of this assignment are:

- Transform a real-world feature using a nonlinear function
- Assume the transformed data follows an **unknown probability distribution**
- Design and train a **GAN** to learn this distribution
- Approximate the learned PDF using samples generated by the generator

---

## ğŸ“Š Dataset

- **Dataset Name:** India Air Quality Data  
- **Source:** Kaggle  
- **Link:** https://www.kaggle.com/datasets/shrutibhargava94/india-air-quality-data  
- **Feature Used:** NOâ‚‚ (Nitrogen Dioxide) concentration  
- **File Used:** `data.csv`

The dataset is downloaded **programmatically using the Kaggle API** to ensure reproducibility.

---

## ğŸ” Step-1: Data Transformation

Each NOâ‚‚ concentration value \(x\) is transformed using the following nonlinear function:

\[
z = \mathrm{Tr}(x) = x + a_r \sin(b_r x)
\]

### Transformation Parameters

\[
a_r = 0.5 \times (r \bmod 7)
\]

\[
b_r = 0.3 \times ((r \bmod 5) + 1)
\]

Where:
- \(r\) is 102316044

This transformation introduces **nonlinearity and multimodal structure**, making the analytical PDF of \(z\) intractable.

---

## ğŸ¤– Step-2: Learning the PDF Using GAN

Since the analytical form of the PDF is unknown, a **Generative Adversarial Network (GAN)** is used to model the distribution implicitly.

### GAN Components

#### Generator (G)
- Input: Random noise \( \epsilon \sim \mathcal{N}(0,1) \)
- Output: Generated samples \( z_f = G(\epsilon) \)
- Objective: Fool the discriminator by generating realistic samples

#### Discriminator (D)
- Input: Real samples \(z\) and fake samples \(z_f\)
- Output: Probability that a sample is real
- Objective: Distinguish between real and generated samples

---

### GAN Architecture

#### Generator Network
| Layer | Units | Activation |
|------|------|-----------|
| Input | 1 | â€” |
| Dense | 64 | ReLU |
| Dense | 64 | ReLU |
| Output | 1 | Linear |

#### Discriminator Network
| Layer | Units | Activation |
|------|------|-----------|
| Input | 1 | â€” |
| Dense | 64 | ReLU |
| Dense | 64 | ReLU |
| Output | 1 | Sigmoid |

---

### Loss Functions

**Discriminator Loss**
\[
\mathcal{L}_D =
- \mathbb{E}[\log D(z)]
- \mathbb{E}[\log(1 - D(G(\epsilon)))]
\]

**Generator Loss**
\[
\mathcal{L}_G =
- \mathbb{E}[\log D(G(\epsilon))]
\]

Binary cross-entropy loss is used for training both networks.

---

## ğŸ“ˆ Step-3: PDF Approximation from Generator Samples

After training the GAN:

1. A large number of samples are generated from the generator
2. The probability density function is approximated using:
   - **Kernel Density Estimation (KDE)**

\[
\hat{p}_h(z) =
\frac{1}{Nh}
\sum_{i=1}^{N}
K\left(\frac{z - z_f^{(i)}}{h}\right)
\]

The KDE of generated samples is compared with the KDE of real transformed data.

---

## ğŸ“Š Results

- The GAN successfully learns the unknown distribution of the transformed variable
- KDE plots of real and generated samples show strong overlap
- Multimodal behavior introduced by the nonlinear transformation is captured effectively

---

## ğŸ“ Observations

### Mode Coverage
- The generator captures multiple modes present in the transformed data
- Minor mode dropping may occur during early stages of training

### Training Stability
- Loss curves show oscillatory behavior, which is expected in GAN training
- Stable learning achieved using a small learning rate

### Quality of Generated Distribution
- Generated samples closely resemble real data
- KDE confirms accurate approximation of the underlying PDF
